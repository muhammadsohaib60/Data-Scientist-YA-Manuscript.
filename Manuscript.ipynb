{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcRyCn8NPYT7",
        "outputId": "24d8f6da-1806-43fd-e983-acacde503745"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from docx) (5.3.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.10/dist-packages (from docx) (11.0.0)\n",
            "Building wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53892 sha256=bd9fa53fa773933d3f1233127a04b6401da8dd5d56c2b0d7eb5b18753319cbab\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/f5/1d/e09ba2c1907a43a4146d1189ae4733ca1a3bfe27ee39507767\n",
            "Successfully built docx\n",
            "Installing collected packages: docx\n",
            "Successfully installed docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KkIhqy9P4vt",
        "outputId": "d07ef735-12f2-422f-e854-34121f6179c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m225.3/244.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJn1CK5VQD9v",
        "outputId": "9fa8c74a-31ea-4ad3-fdc2-036e8728f902"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "g6K09OXqPCty"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "from docx import Document\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure WordNet is downloaded (for synonym generation)\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DloxqJ5mPwdG",
        "outputId": "d67aeed6-7615-4b16-da26-0501022063a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load spaCy model for NLP processing\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "RyBgl783QQ3F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get synonyms using WordNet\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)"
      ],
      "metadata": {
        "id": "KQ48VCPpQwvt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to analyze and suggest simpler words\n",
        "def simplify_terms(text):\n",
        "    doc = nlp(text)\n",
        "    simplified_text = text\n",
        "\n",
        "    # Iterate over tokens and replace complex words with simpler synonyms\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['NOUN', 'VERB', 'ADJ']:  # Simplify nouns, verbs, and adjectives\n",
        "            synonyms = get_synonyms(token.text)\n",
        "            if len(synonyms) > 0:\n",
        "                simplified_word = min(synonyms, key=len)  # Choose the simplest synonym (shortest)\n",
        "                # Ensure context correctness by checking synonym similarity\n",
        "                if simplified_word.lower() != token.text.lower():\n",
        "                    simplified_text = simplified_text.replace(token.text, simplified_word)\n",
        "\n",
        "    return simplified_text"
      ],
      "metadata": {
        "id": "CBdXdH5-Qza6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to identify key data science concepts in the manuscript\n",
        "def identify_data_science_terms(text):\n",
        "    # Example list of technical terms to identify and simplify\n",
        "    data_science_terms = [\n",
        "        \"algorithm\", \"data frame\", \"machine learning\", \"neural network\", \"regression\",\n",
        "        \"classification\", \"statistics\", \"supervised learning\", \"unsupervised learning\",\n",
        "        \"overfitting\", \"bias\", \"variance\", \"clustering\", \"data set\", \"feature engineering\",\n",
        "        \"predictive model\", \"model training\", \"decision tree\", \"k-means\", \"support vector machine\",\n",
        "        \"cross-validation\", \"ensemble method\", \"deep learning\", \"reinforcement learning\", \"natural language processing\",\n",
        "        \"convolutional neural network\", \"gradient descent\"\n",
        "    ]\n",
        "\n",
        "    found_terms = []\n",
        "    for term in data_science_terms:\n",
        "        if re.search(r'\\b' + re.escape(term) + r'\\b', text.lower()):\n",
        "            found_terms.append(term)\n",
        "\n",
        "    return found_terms"
      ],
      "metadata": {
        "id": "N3ZkvQ0LQ4PS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read text from a DOCX file\n",
        "def read_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return text\n",
        "\n",
        "# Function to read text from a PDF file\n",
        "def read_pdf(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Function to handle multiple input formats (Text, DOCX, PDF)\n",
        "def read_manuscript(file_path):\n",
        "    if file_path.endswith('.txt'):\n",
        "        with open(file_path, 'r') as file:\n",
        "            return file.read()\n",
        "    elif file_path.endswith('.docx'):\n",
        "        return read_docx(file_path)\n",
        "    elif file_path.endswith('.pdf'):\n",
        "        return read_pdf(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a .txt, .docx, or .pdf file.\")\n"
      ],
      "metadata": {
        "id": "m5X87llxRE3j"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to generate feedback report\n",
        "def generate_feedback_report(text):\n",
        "    # Identify key data science terms in the manuscript\n",
        "    found_terms = identify_data_science_terms(text)\n",
        "\n",
        "    # Generate simplified text for easier understanding\n",
        "    simplified_text = simplify_terms(text)\n",
        "\n",
        "    # Create feedback report\n",
        "    feedback = {\n",
        "        \"identified_terms\": found_terms,\n",
        "        \"simplified_text\": simplified_text,\n",
        "        \"overall_feedback\": \"The manuscript contains complex technical terms which have been simplified for a young audience. Review the simplified text for accuracy and relevance.\"\n",
        "    }\n",
        "\n",
        "    return feedback"
      ],
      "metadata": {
        "id": "6F70s6EyRV9r"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract key sentences with complex terminology\n",
        "def extract_complex_sentences(text):\n",
        "    complex_sentences = []\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Identify sentences that contain data science terms\n",
        "    for sentence in sentences:\n",
        "        if any(term.lower() in sentence.lower() for term in [\"algorithm\", \"neural network\", \"overfitting\", \"model\", \"machine learning\"]):\n",
        "            complex_sentences.append(sentence)\n",
        "\n",
        "    return complex_sentences"
      ],
      "metadata": {
        "id": "q6qnCI-eRYuK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to suggest more relatable examples for technical terms\n",
        "def suggest_examples_for_terms(text):\n",
        "    term_examples = {\n",
        "        \"neural network\": \"Think of it as a brain trying to recognize objects or patterns from a picture, just like how humans learn.\",\n",
        "        \"overfitting\": \"Imagine a teacher who teaches only one specific question over and over again. The student might do well on that question but fail others.\",\n",
        "        \"classification\": \"It's like sorting items into different boxes based on their characteristics, like separating animals into cats, dogs, and birds.\",\n",
        "        \"regression\": \"This is like trying to predict how much someone will score on a test based on how much they studied.\"\n",
        "    }\n",
        "\n",
        "    for term, example in term_examples.items():\n",
        "        text = text.replace(term, f\"{term} (For example: {example})\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "XPiUfuB0RbC0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to process the manuscript, extract terms, simplify text, and provide feedback\n",
        "def process_manuscript(file_path):\n",
        "    text = read_manuscript(file_path)\n",
        "\n",
        "    # Generate the feedback report\n",
        "    feedback = generate_feedback_report(text)\n",
        "\n",
        "    # Extract complex sentences\n",
        "    complex_sentences = extract_complex_sentences(text)\n",
        "\n",
        "    # Suggest examples for technical terms\n",
        "    enhanced_text = suggest_examples_for_terms(feedback['simplified_text'])\n",
        "\n",
        "    # Display the results\n",
        "    print(\"Identified Data Science Terms:\")\n",
        "    print(feedback['identified_terms'])\n",
        "\n",
        "    print(\"\\nSimplified Text:\")\n",
        "    print(feedback['simplified_text'])\n",
        "\n",
        "    print(\"\\nComplex Sentences in the Manuscript:\")\n",
        "    for sentence in complex_sentences:\n",
        "        print(f\"- {sentence}\")\n",
        "\n",
        "    print(\"\\nEnhanced Text with Examples for Technical Terms:\")\n",
        "    print(enhanced_text)\n",
        "\n",
        "    print(\"\\nOverall Feedback:\")\n",
        "    print(feedback['overall_feedback'])\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Provide the file path to the manuscript\n",
        "    file_path = \"manuscript.pdf\""
      ],
      "metadata": {
        "id": "XhS11pdcRlA1"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}